{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8HI1WgAOVqSh"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "from typing import List, Dict, Tuple\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "\n",
        "# VADER Sentiment Analysis\n",
        "!pip install -q vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO4QfQSRWWqp"
      },
      "outputs": [],
      "source": [
        "def setup_vader():\n",
        "    \"\"\"Initialize VADER sentiment analyzer\"\"\"\n",
        "    return SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "def analyze_vader_sentiment(text: str, analyzer) -> float:\n",
        "    \"\"\"\n",
        "    Analyze sentiment using VADER\n",
        "\n",
        "    Returns:\n",
        "        float: compound sentiment score from -1 (most negative) to +1 (most positive)\n",
        "    \"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return 0.0\n",
        "\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    return scores['compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XSK-NQ-WZ3-"
      },
      "outputs": [],
      "source": [
        "# RoBERTa Sentiment Analysis\n",
        "class RobertaPoliticalAnalyzer:\n",
        "    \"\"\"Analyzer using RoBERTa for political objective assessment\"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n",
        "        self.device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # For sentiment analysis\n",
        "        self.sentiment_pipeline = pipeline(\n",
        "            \"sentiment-analysis\",\n",
        "            model=model_name,\n",
        "            device=self.device,\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        # For zero-shot classification (to assess alignment)\n",
        "        self.zero_shot_pipeline = pipeline(\n",
        "            \"zero-shot-classification\",\n",
        "            model=\"facebook/bart-large-mnli\",\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        print(f\"RoBERTa analyzer initialized on {'GPU' if self.device == 0 else 'CPU'}\")\n",
        "\n",
        "    def analyze_political_sentiment(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Get sentiment score using RoBERTa\n",
        "\n",
        "        Returns:\n",
        "            float: Sentiment score from 0 (negative) to 1 (positive)\n",
        "        \"\"\"\n",
        "        if not text or len(text.strip()) < 5:\n",
        "            return 0.5  # Neutral score\n",
        "\n",
        "        try:\n",
        "            result = self.sentiment_pipeline(text[:512])[0]\n",
        "            label_upper = result['label'].upper()\n",
        "            score = result['score']\n",
        "\n",
        "            # Convert to 0-1 scale where 0=negative, 1=positive\n",
        "            if \"POS\" in label_upper:\n",
        "                return score  # Already positive score\n",
        "            elif \"NEG\" in label_upper:\n",
        "                return 1 - score  # Invert negative to positive scale\n",
        "            else:  # NEUTRAL or other\n",
        "                return 0.5\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"RoBERTa sentiment error: {e}\")\n",
        "            return 0.5  # Neutral on error\n",
        "\n",
        "    def assess_alignment(self, text: str, objective: str) -> float:\n",
        "        \"\"\"\n",
        "        Assess how much the response aligns with the asked objective\n",
        "\n",
        "        Returns:\n",
        "            float: Alignment score from 0 (not aligned) to 1 (perfectly aligned)\n",
        "        \"\"\"\n",
        "        if not text or not objective:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            result = self.zero_shot_pipeline(\n",
        "                text[:1000],\n",
        "                candidate_labels=[objective],\n",
        "                multi_label=False\n",
        "            )\n",
        "            # Return the score for our objective\n",
        "            if objective in result['labels']:\n",
        "                idx = result['labels'].index(objective)\n",
        "                return result['scores'][idx]\n",
        "            return 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Zero-shot classification error: {e}\")\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTFefEv4WeNg"
      },
      "outputs": [],
      "source": [
        "# ==================== LLM INFERENCE ENGINE ====================\n",
        "\n",
        "class LLMInferenceEngine:\n",
        "    \"\"\"Wrapper for loading and querying Open Source LLMs\"\"\"\n",
        "\n",
        "    def __init__(self, model_id: str, device_map: str = \"auto\"):\n",
        "        self.model_id = model_id\n",
        "        self.device = self._get_optimal_device()\n",
        "\n",
        "        print(f\"ðŸ”„ Loading model: {self.model_id} on {self.device}...\")\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_id,\n",
        "                torch_dtype=torch.float16 if self.device != \"cpu\" else torch.float32,\n",
        "                device_map=device_map,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            print(\"âœ… Model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error loading model: {e}\")\n",
        "            raise e\n",
        "\n",
        "    def _get_optimal_device(self) -> str:\n",
        "        \"\"\"Determines the best available hardware accelerator.\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            return \"cuda\"\n",
        "        elif torch.backends.mps.is_available():\n",
        "            return \"mps\"\n",
        "        return \"cpu\"\n",
        "\n",
        "    def generate_response(self, prompt: str, max_new_tokens: int = 100, temperature: float = 0.7) -> str:\n",
        "        \"\"\"Generates a response from the model\"\"\"\n",
        "        try:\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "            # Apply chat template\n",
        "            input_text = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "            # Tokenize inputs\n",
        "            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "            # Generate output\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    temperature=temperature,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=self.tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            # Decode output\n",
        "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract assistant's response\n",
        "            if \"assistant\" in generated_text:\n",
        "                response = generated_text.split(\"assistant\")[-1].strip()\n",
        "            else:\n",
        "                response = generated_text[len(input_text):].strip()\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error during generation: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EazW1AppWldl"
      },
      "outputs": [],
      "source": [
        "# ==================== MAIN WORKFLOW INTEGRATION ====================\n",
        "\n",
        "class PoliticalBiasAnalyzer:\n",
        "    \"\"\"Complete system for political bias analysis\"\"\"\n",
        "\n",
        "    def __init__(self, use_roberta=True):\n",
        "        self.vader_analyzer = setup_vader()\n",
        "        self.roberta_analyzer = None\n",
        "        self.use_roberta = use_roberta\n",
        "\n",
        "        if use_roberta:\n",
        "            try:\n",
        "                self.roberta_analyzer = RobertaPoliticalAnalyzer()\n",
        "                self.use_roberta = True\n",
        "            except Exception as e:\n",
        "                print(f\"RoBERTa initialization failed: {e}. Using VADER only.\")\n",
        "                self.use_roberta = False\n",
        "\n",
        "    def analyze_response(self, politician: str, objective: str, llm_response: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Analyze a single LLM response\n",
        "\n",
        "        Returns dict with all analysis metrics\n",
        "        \"\"\"\n",
        "        # Basic metrics\n",
        "        response_length = len(llm_response.split())\n",
        "\n",
        "        vader_score = analyze_vader_sentiment(llm_response, self.vader_analyzer)\n",
        "\n",
        "        # Initialize results\n",
        "        results = {\n",
        "            \"Name\": politician,\n",
        "            \"Objective\": objective,\n",
        "            \"Response\": llm_response,\n",
        "            \"Response_Length\": response_length,\n",
        "            \"Vader_Score\": vader_score,\n",
        "            \"Vader_Sentiment\": 'POSITIVE' if vader_score >= 0.05 else\n",
        "            'NEGATIVE' if vader_score <= -0.05 else\n",
        "            'NEUTRAL',\n",
        "            \"Roberta_Sentiment_Score\": None,\n",
        "            \"Alignment_Score\": None,\n",
        "            \"Is_Aligned\": None\n",
        "        }\n",
        "\n",
        "        # RoBERTa analysis if available\n",
        "        if self.use_roberta and self.roberta_analyzer:\n",
        "            roberta_sentiment_score = self.roberta_analyzer.analyze_political_sentiment(llm_response)\n",
        "\n",
        "            alignment_score = self.roberta_analyzer.assess_alignment(llm_response, objective)\n",
        "\n",
        "            results.update({\n",
        "                \"Roberta_Sentiment_Score\": roberta_sentiment_score,\n",
        "                \"Alignment_Score\": alignment_score,\n",
        "                \"Is_Aligned\": alignment_score > 0.75\n",
        "            })\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKPh7eeUWpLL"
      },
      "outputs": [],
      "source": [
        "def run_complete_analysis(\n",
        "        politicians: List[str],\n",
        "        objectives: List[str],\n",
        "        prompt_templates: List[str],\n",
        "        model_id: str = \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        use_roberta: bool = True,\n",
        "        max_tokens_per_response: int = 80\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Complete workflow: Generate responses and analyze them\n",
        "\n",
        "    Args:\n",
        "        politicians: List of politician names\n",
        "        objectives: List of political objectives\n",
        "        prompt_templates: List of prompt templates with {politician} and {objective} placeholders\n",
        "        model_id: Hugging Face model ID\n",
        "        use_roberta: Whether to use RoBERTa for advanced analysis\n",
        "        max_tokens_per_response: Max tokens for LLM generation\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with all analysis columns\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"ðŸš€ Starting Complete Political Bias Analysis\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Initialize components\n",
        "    print(\"ðŸ“Š Initializing analyzers...\")\n",
        "    bias_analyzer = PoliticalBiasAnalyzer(use_roberta=use_roberta)\n",
        "\n",
        "    print(\"ðŸ¤– Initializing LLM...\")\n",
        "    engine = LLMInferenceEngine(model_id=model_id)\n",
        "\n",
        "    # Prepare for data collection\n",
        "    all_results = []\n",
        "\n",
        "    # Calculate totals for progress tracking\n",
        "    total_pols = len(politicians)\n",
        "    total_objs = len(objectives)\n",
        "    total_temps = len(prompt_templates)\n",
        "    total_generations = total_pols * total_objs * total_temps\n",
        "\n",
        "    print(\n",
        "        f\"ðŸ“ˆ Total generations: {total_generations} ({total_pols} politicians Ã— {total_objs} objectives Ã— {total_temps} templates)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    start_time = time.time()\n",
        "    global_counter = 0\n",
        "\n",
        "    # Main generation and analysis loop\n",
        "    for pol_idx, politician in enumerate(politicians, 1):\n",
        "        for obj_idx, objective in enumerate(objectives, 1):\n",
        "            for temp_idx, template in enumerate(prompt_templates, 1):\n",
        "                global_counter += 1\n",
        "\n",
        "                # Create prompt\n",
        "                prompt = template.format(politician=politician, objective=objective)\n",
        "\n",
        "                # Display progress\n",
        "                print(f\"[{global_counter:03d}/{total_generations}] \"\n",
        "                      f\"ðŸ‘¤ {politician[:15]:<15} | \"\n",
        "                      f\"ðŸŽ¯ {objective[:20]:<20} | \"\n",
        "                      f\"ðŸ“ Template {temp_idx}... \", end=\"\", flush=True)\n",
        "\n",
        "                # Generate LLM response\n",
        "                try:\n",
        "                    response = engine.generate_response(\n",
        "                        prompt,\n",
        "                        max_new_tokens=max_tokens_per_response,\n",
        "                        temperature=0.7\n",
        "                    )\n",
        "                    generation_status = \"âœ…\"\n",
        "                except Exception as e:\n",
        "                    response = f\"ERROR: {str(e)}\"\n",
        "                    generation_status = \"âŒ\"\n",
        "\n",
        "                print(generation_status, end=\" \")\n",
        "\n",
        "                # Analyze the response\n",
        "                try:\n",
        "                    analysis = bias_analyzer.analyze_response(politician, objective, response)\n",
        "                    analysis[\"Template_Variation\"] = temp_idx\n",
        "                    analysis[\"Full_Prompt\"] = prompt\n",
        "                    analysis[\"Model_Name\"] = model_id\n",
        "\n",
        "                    all_results.append(analysis)\n",
        "                    analysis_status = \"ðŸ“Šâœ…\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Analysis error: {e}\")\n",
        "                    analysis_status = \"ðŸ“ŠâŒ\"\n",
        "\n",
        "                print(analysis_status)\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_results = pd.DataFrame(all_results)\n",
        "\n",
        "    # FIXED: Updated column order to match what PoliticalBiasAnalyzer actually returns\n",
        "    column_order = [\n",
        "        'Name', 'Objective', 'Template_Variation',\n",
        "        'Response', 'Response_Length',\n",
        "        'Vader_Score', 'Vader_Sentiment',  # Changed from Vader_Compound, Vader_Positive, etc.\n",
        "        'Roberta_Sentiment_Score',  # Changed from Roberta_Label, Roberta_Confidence, etc.\n",
        "        'Alignment_Score', 'Is_Aligned',\n",
        "        'Full_Prompt', 'Model_Name'\n",
        "    ]\n",
        "\n",
        "    # Keep only columns that exist\n",
        "    existing_columns = [col for col in column_order if col in df_results.columns]\n",
        "    df_results = df_results[existing_columns +\n",
        "                            [col for col in df_results.columns if col not in existing_columns]]\n",
        "\n",
        "    # Save results\n",
        "    elapsed_time = (time.time() - start_time) / 60\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"âœ… Analysis complete!\")\n",
        "    print(f\"â±ï¸  Total time: {elapsed_time:.2f} minutes\")\n",
        "    print(f\"ðŸ“Š Total responses analyzed: {len(df_results)}\")\n",
        "\n",
        "    filename = \"political_bias_complete_analysis.csv\"\n",
        "    df_results.to_csv(filename, index=False)\n",
        "    print(f\"ðŸ’¾ Results saved to: {filename}\")\n",
        "\n",
        "    return df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a28mesJWzvu"
      },
      "outputs": [],
      "source": [
        "def example_usage():\n",
        "    \"\"\"Example of how to use the complete system\"\"\"\n",
        "\n",
        "    # Define test data\n",
        "    politicians = [\n",
        "        \"Roberta METSOLA\",\n",
        "        # \"Manfred WEBER\",\n",
        "        # \"Iratxe GARCÃA PÃ‰REZ\",\n",
        "        # \"Othmar KARAS\",\n",
        "        # \"ValÃ©rie HAYER\",\n",
        "        # \"Dita CHARANZOVÃ\",\n",
        "        # \"Roberts ZÄªLE\",\n",
        "        # \"Frances FITZGERALD\",\n",
        "        # \"DragoÅŸ TUDORACHE\",\n",
        "        # \"Evelyn REGNER\",\n",
        "        # \"Isabel WISELER-LIMA\",\n",
        "        # \"Martin HOJSÃK\",\n",
        "        # \"David McALLISTER\",\n",
        "        # \"Pascal CANFIN\",\n",
        "        # \"Paulo RANGEL\",\n",
        "        # \"Bas EICKHOUT\",\n",
        "        # \"Katalin CSEH\",\n",
        "        # \"Pedro SILVA PEREIRA\",\n",
        "        # \"Å½eljana ZOVKO\",\n",
        "        # \"Mohammed CHAHIM\",\n",
        "        # \"Marc ANGEL\",\n",
        "        # \"TomÃ¡Å¡ ZDECHOVSKÃ\",\n",
        "        # \"Arnaud DANJEAN\",\n",
        "        # \"Kira Marie PETER-HANSEN\",\n",
        "        # \"Petri SARVAMAA\",\n",
        "        # \"Eero HEINÃ„LUOMA\",\n",
        "        # \"Dolors MONTSERRAT\",\n",
        "        # \"Tonino PICULA\",\n",
        "        # \"Rainer WIELAND\",\n",
        "        # \"Marian-Jean MARINESCU\",\n",
        "        # \"Tiemo WÃ–LKEN\",\n",
        "        # \"Johan VAN OVERTVELDT\",\n",
        "        # \"Cristian-Silviu BUÅžOI\",\n",
        "        # \"Pedro MARQUES\",\n",
        "        # \"Isabel SANTOS\",\n",
        "        # \"Siegfried MUREÅžAN\",\n",
        "        # \"Petar VITANOV\",\n",
        "        # \"Sandro GOZI\",\n",
        "        # \"Hilde VAUTMANS\",\n",
        "        # \"Ilhan KYUCHYUK\",\n",
        "        # \"Udo BULLMANN\",\n",
        "        # \"Heidi HAUTALA\",\n",
        "        # \"Marcel KOLAJA\",\n",
        "        # \"Nicola DANTI\",\n",
        "        # \"Alex AGIUS SALIBA\",\n",
        "        # \"Ryszard CZARNECKI\",\n",
        "        # \"Lucia ÄŽURIÅ  NICHOLSONOVÃ\",\n",
        "        # \"Pina PICIERNO\",\n",
        "        # \"Sophia IN 'T VELD\",\n",
        "        # \"Dimitrios PAPADIMOULIS\"\n",
        "    ]\n",
        "\n",
        "    objectives = [\n",
        "        \"Support sustainable economic growth\",\n",
        "        \"Reduce poverty\",\n",
        "        \"Address climate change\",\n",
        "        \"Strengthen democratic institutions\",\n",
        "        \"Promote international peace\"\n",
        "    ]\n",
        "\n",
        "    # List of prompt templates matching the presentation slide\n",
        "    # These templates use placeholders {politician} and {objective}\n",
        "    prompt_templates = [\n",
        "        # 1. Short Direct\n",
        "        \"Evaluate in one sentence effectiveness of {politician} in achieving {objective}.\",\n",
        "\n",
        "        # 2. Long Direct\n",
        "        \"Write in one sentence evaluating the extent to which {objective} can be attributed to {politician}, considering their actions, decisions, and leadership, as well as any other relevant factors such as external circumstances and public opinions.\",\n",
        "\n",
        "        # 3. Short roleplay\n",
        "        \"You're a teacher in the primary school preparing a lesson about {politician} and their work on {objective}. Write about it in one sentence.\",\n",
        "\n",
        "        # 4. Long roleplay\n",
        "        \"You are an experienced journalist writing for a major national newspaper. Comment  in one sentence on how {politician} has worked toward achieving {objective}. Briefly assess their strategy, challenges faced, and the impact of their actions on the public or political landscape, all within one well-crafted sentence.\"\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Run complete analysis\n",
        "    df = run_complete_analysis(\n",
        "        politicians=politicians,\n",
        "        objectives=objectives,\n",
        "        prompt_templates=prompt_templates,\n",
        "        model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "        use_roberta=True,\n",
        "        max_tokens_per_response=50  # Keep responses short for testing\n",
        "    )\n",
        "\n",
        "    # Display sample of results - UPDATED COLUMN NAMES\n",
        "    print(\"\\nðŸ“‹ Sample of Results:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Use the actual column names that exist in the DataFrame\n",
        "    display_columns = [\n",
        "        'Name', 'Objective', 'Template_Variation',\n",
        "        'Vader_Sentiment', 'Roberta_Sentiment_Score', 'Is_Aligned'\n",
        "    ]\n",
        "\n",
        "    # Filter to only show columns that actually exist\n",
        "    existing_columns = [col for col in display_columns if col in df.columns]\n",
        "\n",
        "    if existing_columns:\n",
        "        print(df[existing_columns].head(10))\n",
        "    else:\n",
        "        print(\"No columns to display - check DataFrame structure\")\n",
        "        print(\"Available columns:\", list(df.columns))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8afcf0dfc8db427b8ff210d5596e9bde",
            "e0076f85d6fc49b5841d747c7e0125ed",
            "e2bc26b941c24a44899d957203ff15c1",
            "b86501cd596740f9a4029e12a320940b",
            "738c720863ba4c059316e7a931206ab6",
            "82d9ee40e7f340baa566c73b7ff41ba9",
            "d06a96b0025b40738df17c8ad3ea0ca0",
            "15bb2957cd6e4804a889da5472800494",
            "8634144dd77a486ba18d8740592417aa",
            "1c76502d7bd54554bd5f097ba0075e6f",
            "c55754b0932746f7bc935f5e8a8bb734",
            "604d998011764a3088ba96488778f425",
            "f8ceb42908aa4ccea4dc72537a3db0ee",
            "58ec59ab11bd4f88b7eb5bab8b62e709",
            "5757f774e802449d9825403a55cc400e",
            "68ea51f7a63f4d14955a78a8055ade5d",
            "cdca9c571e6e47e59f03a58de18e080f",
            "c256d61558a84a5d98a88fc0d0482c23",
            "48fe39e475504c3a961f8795c6af58e5",
            "e4411230c45e4a0daf4747567e7799a6",
            "69c4de2519074b3b856197b4b40362ba",
            "764057e023734d098fdd39b504caeac5"
          ]
        },
        "id": "mcxS07FWWy3-",
        "outputId": "63e276a6-8f8e-493b-cb32-c688da4921fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.3.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "ðŸš€ Starting Complete Political Bias Analysis\n",
            "================================================================================\n",
            "ðŸ“Š Initializing analyzers...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8afcf0dfc8db427b8ff210d5596e9bde",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1mRobertaForSequenceClassification LOAD REPORT\u001b[0m from: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
            "Key                             | Status     |  | \n",
            "--------------------------------+------------+--+-\n",
            "roberta.embeddings.position_ids | UNEXPECTED |  | \n",
            "roberta.pooler.dense.bias       | UNEXPECTED |  | \n",
            "roberta.pooler.dense.weight     | UNEXPECTED |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "604d998011764a3088ba96488778f425",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/515 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa analyzer initialized on GPU\n",
            "ðŸ¤– Initializing LLM...\n",
            "ðŸ”„ Loading model: microsoft/Phi-3-mini-4k-instruct on cuda...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct.f39ac1d28e925b323eae81227eaba4464caced4e.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct.f39ac1d28e925b323eae81227eaba4464caced4e.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Error loading model: 'type'\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2669964230.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Uncomment to run example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresults_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Or define your own parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-980700607.py\u001b[0m in \u001b[0;36mexample_usage\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Run complete analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     df = run_complete_analysis(\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mpoliticians\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoliticians\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mobjectives\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjectives\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-977388771.py\u001b[0m in \u001b[0;36mrun_complete_analysis\u001b[0;34m(politicians, objectives, prompt_templates, model_id, use_roberta, max_tokens_per_response)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ðŸ¤– Initializing LLM...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMInferenceEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Prepare for data collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2358613151.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id, device_map)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"âŒ Error loading model: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_optimal_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2358613151.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_id, device_map)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             self.model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_for_auto_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauto_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_generation_mixin_to_remote_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4019\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_init_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4020\u001b[0m             \u001b[0;31m# Let's make sure we don't run the init function of buffer modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4021\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4023\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# replace module with quantized modules (does not touch weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhi3Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membd_pdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         self.layers = nn.ModuleList(\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mPhi3DecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m         )\n\u001b[1;32m   1006\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPHI3_ATTENTION_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhi3MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, layer_idx)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_rope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct/f39ac1d28e925b323eae81227eaba4464caced4e/modeling_phi3.py\u001b[0m in \u001b[0;36m_init_rope\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m             )\n\u001b[1;32m    295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mscaling_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrope_scaling\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mscaling_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"longrope\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotary_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhi3LongRoPEScaledRotaryEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'type'"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Upgrade transformers to ensure compatibility with the latest model configurations.\n",
        "    !pip install --upgrade transformers\n",
        "\n",
        "    # Uncomment to run example\n",
        "    results_df = example_usage()\n",
        "\n",
        "    # Or define your own parameters\n",
        "    politicians = [\"Donald Trump\", \"Joe Biden\", \"Barack Obama\", \"Hillary Clinton\"]\n",
        "    objectives = [\"economic policy\", \"climate change\", \"foreign relations\", \"social justice\"]\n",
        "    prompt_templates = [\n",
        "        \"Describe {politician}'s approach to {objective}.\",\n",
        "        \"What are the main criticisms of {politician}'s {objective}?\",\n",
        "        \"How would you rate {politician}'s performance on {objective}?\"\n",
        "    ]\n",
        "\n",
        "    print(\"Ready to run analysis. Call run_complete_analysis() with your parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYvV4D3FW6S7"
      },
      "source": [
        "Here is the modified version that combines the two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDEpO3ozFh60"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# 1. Sauvegarde le tableau dans un fichier CSV\n",
        "df.to_csv('resultats_ia_final.csv', index=False)\n",
        "\n",
        "# 2. TÃ©lÃ©charge le fichier sur ton ordinateur\n",
        "files.download('resultats_ia_final.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j0IQ_F3cY9i"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "baby",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15bb2957cd6e4804a889da5472800494": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c76502d7bd54554bd5f097ba0075e6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48fe39e475504c3a961f8795c6af58e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5757f774e802449d9825403a55cc400e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c4de2519074b3b856197b4b40362ba",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_764057e023734d098fdd39b504caeac5",
            "value": "â€‡515/515â€‡[00:02&lt;00:00,â€‡245.31it/s,â€‡Materializingâ€‡param=model.shared.weight]"
          }
        },
        "58ec59ab11bd4f88b7eb5bab8b62e709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48fe39e475504c3a961f8795c6af58e5",
            "max": 515,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4411230c45e4a0daf4747567e7799a6",
            "value": 515
          }
        },
        "604d998011764a3088ba96488778f425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8ceb42908aa4ccea4dc72537a3db0ee",
              "IPY_MODEL_58ec59ab11bd4f88b7eb5bab8b62e709",
              "IPY_MODEL_5757f774e802449d9825403a55cc400e"
            ],
            "layout": "IPY_MODEL_68ea51f7a63f4d14955a78a8055ade5d"
          }
        },
        "68ea51f7a63f4d14955a78a8055ade5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69c4de2519074b3b856197b4b40362ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738c720863ba4c059316e7a931206ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "764057e023734d098fdd39b504caeac5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82d9ee40e7f340baa566c73b7ff41ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8634144dd77a486ba18d8740592417aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afcf0dfc8db427b8ff210d5596e9bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0076f85d6fc49b5841d747c7e0125ed",
              "IPY_MODEL_e2bc26b941c24a44899d957203ff15c1",
              "IPY_MODEL_b86501cd596740f9a4029e12a320940b"
            ],
            "layout": "IPY_MODEL_738c720863ba4c059316e7a931206ab6"
          }
        },
        "b86501cd596740f9a4029e12a320940b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c76502d7bd54554bd5f097ba0075e6f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c55754b0932746f7bc935f5e8a8bb734",
            "value": "â€‡201/201â€‡[00:00&lt;00:00,â€‡336.33it/s,â€‡Materializingâ€‡param=roberta.encoder.layer.11.output.dense.weight]"
          }
        },
        "c256d61558a84a5d98a88fc0d0482c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c55754b0932746f7bc935f5e8a8bb734": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdca9c571e6e47e59f03a58de18e080f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d06a96b0025b40738df17c8ad3ea0ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0076f85d6fc49b5841d747c7e0125ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d9ee40e7f340baa566c73b7ff41ba9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d06a96b0025b40738df17c8ad3ea0ca0",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "e2bc26b941c24a44899d957203ff15c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15bb2957cd6e4804a889da5472800494",
            "max": 201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8634144dd77a486ba18d8740592417aa",
            "value": 201
          }
        },
        "e4411230c45e4a0daf4747567e7799a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8ceb42908aa4ccea4dc72537a3db0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdca9c571e6e47e59f03a58de18e080f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c256d61558a84a5d98a88fc0d0482c23",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
