{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Political Bias Analysis - GPU Optimized\n",
    "\n",
    "**Models tested:** SmolLM2-1.7B, Phi-3-mini, Llama, Qwen, Mistral, Gemma\n",
    "\n",
    "**For Google Colab:** Make sure to enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab only)\n",
    "!pip install -q transformers accelerate vaderSentiment torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== CONFIGURATION ==========\nMODEL_ID = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"  # Change this to test other models\n\n# Alternatives:\n# \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B params\n# \"Qwen/Qwen2.5-1.5B-Instruct\"       # 1.5B params\n# \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # 1.1B params\n# \"google/gemma-2-2b-it\"             # 2B params\n\n# CSV file paths (relative to project root)\nPOLS_CSV = \"../data/input/politicians.csv\"\nOBJS_CSV = \"../data/input/objectives.csv\" \nPROMPTS_CSV = \"../data/input/prompts.csv\"\nOUTPUT_CSV = f\"../data/output/{MODEL_ID.split('/')[-1]}_analysis.csv\"\n\nprint(f\"Model: {MODEL_ID}\")\nprint(f\"Output: {OUTPUT_CSV}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "df_pols = pd.read_csv(POLS_CSV)\n",
    "df_objs = pd.read_csv(OBJS_CSV)\n",
    "df_prompts = pd.read_csv(PROMPTS_CSV)\n",
    "\n",
    "print(f\"üìä Loaded:\")\n",
    "print(f\"   {len(df_pols)} politicians\")\n",
    "print(f\"   {len(df_objs)} objectives\")\n",
    "print(f\"   {len(df_prompts)} prompt templates\")\n",
    "print(f\"   Total generations: {len(df_pols) * len(df_objs) * len(df_prompts)}\")\n",
    "\n",
    "# Preview\n",
    "print(\"\\nPoliticians sample:\")\n",
    "display(df_pols.head(3))\n",
    "print(\"\\nObjectives:\")\n",
    "display(df_objs)\n",
    "print(\"\\nPrompts:\")\n",
    "for i, p in enumerate(df_prompts['prompt'].str[:80], 1):\n",
    "    print(f\"  {i}. {p}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEngine:\n",
    "    \"\"\"Loads and runs inference with LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self, model_id):\n",
    "        print(f\"üîÑ Loading LLM: {model_id}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "        \n",
    "        # Fix padding token if missing\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "            \n",
    "        # Load model with GPU support\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded on {self.model.device}\")\n",
    "        \n",
    "    def generate(self, user_content):\n",
    "        \"\"\"Generate response using chat template\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_content}]\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt_formatted = self.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = self.tokenizer(prompt_formatted, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=150, \n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "        # Decode only new tokens\n",
    "        generated_ids = outputs[0][input_length:]\n",
    "        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    \"\"\"Analyzes responses using VADER + RoBERTa\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # VADER\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # RoBERTa\n",
    "        try:\n",
    "            print(\"üîÑ Loading RoBERTa sentiment analyzer...\")\n",
    "            self.roberta = pipeline(\n",
    "                \"sentiment-analysis\",\n",
    "                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "                device=0 if torch.cuda.is_available() else -1,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "            self.has_roberta = True\n",
    "            print(\"‚úÖ RoBERTa loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  RoBERTa unavailable (using VADER only): {e}\")\n",
    "            self.has_roberta = False\n",
    "\n",
    "    def analyze(self, text):\n",
    "        \"\"\"Analyze text and return (response_type, vader_score, roberta_label, roberta_score)\"\"\"\n",
    "        # Detect refusals\n",
    "        r_type = \"VALID\"\n",
    "        refusal_keywords = [\"cannot fulfill\", \"cannot answer\", \"as an ai\", \"neutral stance\", \"policy prohibits\"]\n",
    "        \n",
    "        if any(k in text.lower() for k in refusal_keywords):\n",
    "            r_type = \"REFUSAL\"\n",
    "        elif len(text) < 5:\n",
    "            r_type = \"GARBAGE\"\n",
    "\n",
    "        # VADER sentiment\n",
    "        vs = self.vader.polarity_scores(text)\n",
    "        \n",
    "        # RoBERTa sentiment\n",
    "        rob_label, rob_score = \"ERR\", 0.0\n",
    "        if self.has_roberta and r_type == \"VALID\":\n",
    "            try:\n",
    "                res = self.roberta(text)[0]\n",
    "                rob_label = res['label']\n",
    "                rob_score = res['score']\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return r_type, vs['compound'], rob_label, rob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and analyzer\n",
    "engine = LLMEngine(MODEL_ID)\n",
    "analyzer = Analyzer()\n",
    "\n",
    "print(\"\\n‚úÖ Ready to run inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "total = len(df_pols) * len(df_objs) * len(df_prompts)\n",
    "pbar = tqdm(total=total, desc=\"Inference\")\n",
    "\n",
    "for _, p in df_pols.iterrows():\n",
    "    name = p['MEP']\n",
    "    party = p.get('EPG', 'N/A')\n",
    "    leaning = p.get('Political Leaning', 'N/A')\n",
    "    \n",
    "    for _, o in df_objs.iterrows():\n",
    "        obj = o['objective']\n",
    "        \n",
    "        for _, pr in df_prompts.iterrows():\n",
    "            template = pr['prompt']\n",
    "            \n",
    "            # Create prompt\n",
    "            user_prompt = template.replace(\"{politician}\", name).replace(\"{objective}\", obj)\n",
    "            \n",
    "            try:\n",
    "                # Generate response\n",
    "                resp = engine.generate(user_prompt)\n",
    "                \n",
    "                # Analyze sentiment\n",
    "                r_type, vader, rob_lbl, rob_scr = analyzer.analyze(resp)\n",
    "                \n",
    "                results.append({\n",
    "                    \"Politician\": name,\n",
    "                    \"Party\": party,\n",
    "                    \"Leaning\": leaning,\n",
    "                    \"Objective\": obj,\n",
    "                    \"Prompt_Template\": template,\n",
    "                    \"Full_Prompt\": user_prompt,\n",
    "                    \"Response\": resp,\n",
    "                    \"Response_Type\": r_type,\n",
    "                    \"Vader_Score\": vader,\n",
    "                    \"Roberta_Label\": rob_lbl,\n",
    "                    \"Roberta_Score\": rob_scr,\n",
    "                    \"Model\": MODEL_ID\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è  Error: {e}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Auto-save every 50 rows\n",
    "            if len(results) % 50 == 0:\n",
    "                pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "                print(f\"\\nüíæ Auto-saved {len(results)} rows\")\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Final save\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"\\n‚úÖ Completed! Saved {len(df_results)} rows to {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "df_results = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "print(f\"Total rows: {len(df_results)}\")\n",
    "print(f\"\\nResponse types:\")\n",
    "print(df_results['Response_Type'].value_counts())\n",
    "\n",
    "print(f\"\\nRoBERTa labels:\")\n",
    "print(df_results['Roberta_Label'].value_counts())\n",
    "\n",
    "print(f\"\\nVADER score stats:\")\n",
    "print(df_results['Vader_Score'].describe())\n",
    "\n",
    "print(f\"\\nSample rows:\")\n",
    "display(df_results[['Politician', 'Leaning', 'Objective', 'Vader_Score', 'Roberta_Label', 'Response']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results (Colab only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download from Colab\n",
    "# from google.colab import files\n",
    "# files.download(OUTPUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}